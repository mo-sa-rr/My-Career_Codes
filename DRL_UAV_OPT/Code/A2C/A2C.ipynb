{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40ba90-8a5e-4865-9d97-928312cdb3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d2f30f4-4239-43a8-ae20-f8dd008fd879",
   "metadata": {
    "id": "Xwjn8wDNLMnL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dea38c8e-cc02-4a6e-99cc-a22c905fd8e5",
   "metadata": {
    "id": "Ja1Kkdl2Qx_F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# all array inputs must be of the type np.array\n",
    "\n",
    "class my_env:\n",
    "    #def __init__(self, uav_number, user_number, size, user_loc, noise, height, transmit_pow, safe_dist): # initializes environment\n",
    "    def __init__(self):\n",
    "        self.uav_number = 2\n",
    "        self.user_number = 5\n",
    "        self.size = np.array([10,10])\n",
    "        self.user_loc = np.array([[-5,-5,5,5,0],[-5,5,-5,5,0]])\n",
    "        self.safe_dist = 5\n",
    "        self.noise = 0.01\n",
    "        self.height = 7\n",
    "        self.transmit_pow = 1\n",
    "        self.observation_space = 2*self.uav_number + self.user_number\n",
    "\n",
    "        ### Parameters\n",
    "        self.actions = 5**(self.uav_number) # 0: 10 units Left , 1: 10 units up , 2: 10 units right, 3: 10 units down, 4: stay\n",
    "        self.user_profile = 1*np.ones(self.user_number)\n",
    "        ###\n",
    "\n",
    "    def my_reward(self, uav_loc, scheduling, user_prof): # calculate reward\n",
    "        num_agents = np.shape(uav_loc)[1]\n",
    "        num_user = np.shape(self.user_loc)[1]\n",
    "\n",
    "        pref = np.zeros((num_agents,num_user))\n",
    "        pref_rate = np.zeros((num_agents,num_user))\n",
    "        for i in range(np.shape(pref)[0]):\n",
    "            for j in range(np.shape(pref)[1]):\n",
    "                interf = 0\n",
    "                for k in range(np.shape(pref)[0]):\n",
    "                    if k != i:\n",
    "                        interf = interf + self.transmit_pow/(self.height**2 + np.linalg.norm(uav_loc[:,k]-self.user_loc[:,j]))\n",
    "                pref[i,j] = user_prof[j]*np.log2(1 + self.transmit_pow/(self.height**2 + np.linalg.norm(uav_loc[:,i]-self.user_loc[:,j]))/(interf+self.noise))\n",
    "                #pref[i,j] = np.log2(1 + self.transmit_pow/(self.height**2 + np.linalg.norm(uav_loc[:,i]-self.user_loc[:,j]))/(interf+self.noise))\n",
    "\n",
    "\n",
    "        ####### ---- penalty\n",
    "        penalty = 0\n",
    "\n",
    "        for i in range(np.shape(pref)[0]):\n",
    "            for k in range(np.shape(pref)[0]):\n",
    "                if k != i:\n",
    "                    penalty = penalty + (np.linalg.norm(uav_loc[:,k]-uav_loc[:,i]) < self.safe_dist)\n",
    "\n",
    "        # reward = np.sum(np.multiply(scheduling,pref)) - np.log2(1 + self.transmit_pow/(self.height**2)/self.noise )*penalty/2\n",
    "        reward = np.sum(np.multiply(scheduling,pref)) - 1000*penalty/2\n",
    "\n",
    "        if penalty != 0:\n",
    "            done =  True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        return reward , done\n",
    "\n",
    "\n",
    "    def next_state(self, action1, uav_loc, user_prof, scheduling):\n",
    "        if action1<5:\n",
    "            action = np.base_repr(action1,base=5,padding = 1)\n",
    "        else :\n",
    "            action = np.base_repr(action1,base=5,padding = 0)\n",
    "\n",
    "        for i in range(len(action)):\n",
    "            if action[i] == \"0\":\n",
    "\n",
    "                uav_loc[:,i] = uav_loc[:,i] + (-2*np.array([1,0]))\n",
    "                uav_loc[0,i] = uav_loc[0,i] + (uav_loc[0,i]<-self.size[0]/2)*(-self.size[0]/2-uav_loc[0,i])\n",
    "\n",
    "            elif action[i] == \"1\":\n",
    "\n",
    "                uav_loc[:,i] = uav_loc[:,i] + (2*np.array([0,1]))\n",
    "                uav_loc[1,i] = uav_loc[1,i] + (uav_loc[1,i]>self.size[1]/2)*(self.size[1]/2-uav_loc[1,i])\n",
    "\n",
    "            elif action[i] == \"2\":\n",
    "\n",
    "                uav_loc[:,i] = uav_loc[:,i] + (2*np.array([1,0]))\n",
    "                uav_loc[0,i] = uav_loc[0,i] + (uav_loc[0,i]>self.size[0]/2)*(self.size[0]/2-uav_loc[0,i])\n",
    "\n",
    "            elif action[i] == \"3\":\n",
    "\n",
    "                uav_loc[:,i] = uav_loc[:,i] + (-2*np.array([0,1]))\n",
    "                uav_loc[1,i] = uav_loc[1,i] + (uav_loc[1,i]<-self.size[1]/2)*(-self.size[1]/2-uav_loc[1,i])\n",
    "\n",
    "            else :\n",
    "                pass\n",
    "\n",
    "        #user_prof = user_prof + (0.1 * ~(np.sum(scheduling,axis=0)>0))\n",
    "        # user_prof = user_prof - (np.min(user_prof)>5)*5\n",
    "        user_prof = 1/(0.9/user_prof + 0.1*np.sum(scheduling,axis=0))\n",
    "\n",
    "        return np.hstack((uav_loc.flatten() , user_prof))\n",
    "\n",
    "    def init_state(self):\n",
    "\n",
    "        while True:\n",
    "            a = np.random.uniform(-self.size[0]/2,self.size[0]/2,size = (1,self.uav_number))\n",
    "            b = np.random.uniform(-self.size[1]/2,self.size[1]/2,size = (1,self.uav_number))\n",
    "            c = np.vstack((a,b))\n",
    "            if np.linalg.norm(c[:,0]-c[:,1])>self.safe_dist:\n",
    "                break\n",
    "        \n",
    " \n",
    "\n",
    "        return np.hstack((a.flatten(),b.flatten(),self.user_profile))\n",
    "\n",
    "\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def schedule(user_prof,user_loc,UAV_loc,transmit_pow,noise,height):\n",
    "\n",
    "\n",
    "    num_agents = np.shape(UAV_loc)[1]\n",
    "    num_user = np.shape(user_loc)[1]\n",
    "\n",
    "    A = cp.Variable((num_agents,num_user))\n",
    "\n",
    "    pref = np.zeros((num_agents,num_user))\n",
    "    for i in range(np.shape(pref)[0]):\n",
    "        for j in range(np.shape(pref)[1]):\n",
    "            interf = 0\n",
    "            for k in range(np.shape(pref)[0]):\n",
    "                if k != i:\n",
    "                    interf = interf + transmit_pow/(height**2 + np.linalg.norm(UAV_loc[:,k]-user_loc[:,j]))\n",
    "            pref[i,j] = user_prof[j]*np.log2(1 + transmit_pow/(height**2 + np.linalg.norm(UAV_loc[:,i]-user_loc[:,j]))/(interf+noise))\n",
    "\n",
    "    objective = cp.Minimize(-1*cp.sum(cp.multiply(A,pref)))\n",
    "\n",
    "    constraints = [A>=0,A<=1]\n",
    "    constraints.append(cp.sum(A,axis = 0) <= 1)\n",
    "    constraints.append(cp.sum(A,axis = 1) <= 1)\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    problem.solve()\n",
    "\n",
    "    return np.round(A.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "226a87ac-2838-4673-b9e3-6bb06a0a5e19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFlGXltnRTI0",
    "outputId": "dc018c48-b7f3-4612-e5d6-8a0905156207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4351a2e-c55f-4744-aed5-ab466b716ebc",
   "metadata": {
    "id": "KcPRGUVgRx9z"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LR = 0.001\n",
    "EPISODES = 10000\n",
    "STEPS_PER_EPISODE = 200\n",
    "\n",
    "QUAD = my_env()\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "# # Define Actor network\n",
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         super(Actor, self).__init__()\n",
    "#         self.fc1 = nn.Linear(state_dim, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 1024)\n",
    "#         self.fc3 = nn.Linear(1024, 1024)  # Single output for continuous action\n",
    "#         self.fc4 = nn.Linear(1024, action_dim)\n",
    "#         self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         x = torch.relu(self.fc1(state))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.relu(self.fc3(x))\n",
    "#         #return self.softmax(F.sigmoid(self.fc4(x)))  # Output probability distribution\n",
    "#         return F.softmax(self.fc4(x), dim = -1)\n",
    "#         #return F.softmax(self.fc4(x), dim=-1)\n",
    "\n",
    "#Define Actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, action_dim)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        return (F.softmax(self.fc2(x),dim = -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define Critic network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 1)  # Output a single value\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)  # State value estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f4a1383-28f3-40b9-96ef-1ccb06a2b857",
   "metadata": {
    "id": "NQMPBucndagr"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize networks on GPU\n",
    "actor = Actor(QUAD.observation_space,QUAD.actions).to(device)\n",
    "critic = Critic(QUAD.observation_space).to(device)\n",
    "\n",
    "# for param in actor.parameters():\n",
    "#     if param.ndimension() > 1:  # Check if it's a weight (not a bias)\n",
    "#         torch.nn.init.constant_(param, 0.0001)\n",
    "\n",
    "actor_prev = Actor(QUAD.observation_space,QUAD.actions).to(device)\n",
    "critic_prev = Critic(QUAD.observation_space).to(device)\n",
    "\n",
    "\n",
    "actor.apply(weight_init)\n",
    "critic.apply(weight_init)\n",
    "\n",
    "# Optimizers\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=LR)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea836e63-bdf9-436e-b8bb-840061d5debf",
   "metadata": {
    "id": "2k1NUgQaTaT0"
   },
   "outputs": [],
   "source": [
    "def weight_change(model1,model2):\n",
    "    weight_diff = 0\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        weight_diff += torch.sum(torch.abs(param1 - param2)).item()\n",
    "    return weight_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09a790ca-9eb5-404d-9b50-158ea2f62c54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "gKvc1SDLRQKT",
    "outputId": "74c50505-5815-48be-f743-6ace9ae3bb86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 10/10000, mean past 10 Reward: -406.10477025462495,actor weights: 210.94754314422607, critic weights: 19237.10738171637, elapsed: 0.16677460670471192 minutes\n",
      "episode 20/10000, mean past 10 Reward: -399.5825826678112,actor weights: 192.43238079547882, critic weights: 15818.251704357564, elapsed: 0.3227917551994324 minutes\n",
      "episode 30/10000, mean past 10 Reward: 598.3431316479354,actor weights: 38.648295886814594, critic weights: 5551.548679187894, elapsed: 0.6767906149228414 minutes\n",
      "episode 40/10000, mean past 10 Reward: 278.4165853431806,actor weights: 46.733669489622116, critic weights: 15391.30018159002, elapsed: 0.998733667532603 minutes\n",
      "episode 50/10000, mean past 10 Reward: 417.21556837131004,actor weights: 1.8870290666818619, critic weights: 11787.791447222233, elapsed: 1.3456382036209107 minutes\n",
      "episode 60/10000, mean past 10 Reward: 242.41206445997477,actor weights: 65.66957950592041, critic weights: 10684.325760006905, elapsed: 1.6496082027753194 minutes\n",
      "episode 70/10000, mean past 10 Reward: 416.3642779657092,actor weights: 4.595104705542326, critic weights: 8836.1811221838, elapsed: 1.9962172945340475 minutes\n",
      "episode 80/10000, mean past 10 Reward: 65.56503230009898,actor weights: 12.85955054871738, critic weights: 8130.088826939464, elapsed: 2.257175044218699 minutes\n",
      "episode 90/10000, mean past 10 Reward: 595.7382109251278,actor weights: 0.7251417171210051, critic weights: 892.1356824040413, elapsed: 2.646321698029836 minutes\n",
      "episode 100/10000, mean past 10 Reward: 419.4921393650135,actor weights: 6.3284680880606174, critic weights: 2749.0287349820137, elapsed: 2.99188525279363 minutes\n",
      "episode 110/10000, mean past 10 Reward: 419.4194743500228,actor weights: 11.041690660640597, critic weights: 4720.40072414279, elapsed: 3.3425580779711406 minutes\n",
      "episode 120/10000, mean past 10 Reward: 241.8095064049986,actor weights: 6.8938859440386295, critic weights: 2191.659154266119, elapsed: 3.648960212866465 minutes\n",
      "episode 130/10000, mean past 10 Reward: 593.648756865735,actor weights: 47.88369497656822, critic weights: 5962.226004213095, elapsed: 4.037606569131215 minutes\n",
      "episode 140/10000, mean past 10 Reward: 270.25992523936367,actor weights: 158.2693738937378, critic weights: 3860.8498228788376, elapsed: 4.357456847031911 minutes\n",
      "episode 150/10000, mean past 10 Reward: 243.70473284207068,actor weights: 85.22592191398144, critic weights: 3461.049726128578, elapsed: 4.661038506031036 minutes\n",
      "episode 160/10000, mean past 10 Reward: 241.45786329369994,actor weights: 72.89970156550407, critic weights: 3582.931072950363, elapsed: 4.964108026027679 minutes\n",
      "episode 170/10000, mean past 10 Reward: 241.62427118543536,actor weights: 39.22621935606003, critic weights: 2513.716102004051, elapsed: 5.267411092917125 minutes\n",
      "episode 180/10000, mean past 10 Reward: 593.5304242141524,actor weights: 30.19079789891839, critic weights: 3206.331657618284, elapsed: 5.657062240441641 minutes\n",
      "episode 190/10000, mean past 10 Reward: 264.82929768365705,actor weights: 87.48269158601761, critic weights: 3982.7997855842113, elapsed: 5.972561979293824 minutes\n",
      "episode 200/10000, mean past 10 Reward: 773.8526619323793,actor weights: 138.87861239910126, critic weights: 1587.4115063548088, elapsed: 6.403731874624888 minutes\n",
      "episode 210/10000, mean past 10 Reward: 594.1143869328351,actor weights: 80.01100593805313, critic weights: 1982.0066353082657, elapsed: 6.794101544221243 minutes\n",
      "episode 220/10000, mean past 10 Reward: 242.475546443684,actor weights: 170.39583563804626, critic weights: 4266.045915007591, elapsed: 7.098848636945089 minutes\n",
      "episode 230/10000, mean past 10 Reward: 417.6916288915475,actor weights: 141.7413169145584, critic weights: 4279.160084724426, elapsed: 7.445130634307861 minutes\n",
      "episode 240/10000, mean past 10 Reward: 618.9253329686823,actor weights: 84.23268729448318, critic weights: 3467.5052611231804, elapsed: 7.848243077596028 minutes\n",
      "episode 250/10000, mean past 10 Reward: 769.7869140231126,actor weights: 16.823622544994578, critic weights: 1778.8527474999428, elapsed: 8.27799458503723 minutes\n",
      "episode 260/10000, mean past 10 Reward: 768.4821916109468,actor weights: 8.102078901603818, critic weights: 810.6547474265099, elapsed: 8.707342553138734 minutes\n",
      "episode 270/10000, mean past 10 Reward: 770.6198537504931,actor weights: 9.287299958523363, critic weights: 662.0375844836235, elapsed: 9.136348315080006 minutes\n",
      "episode 280/10000, mean past 10 Reward: 770.1139859118999,actor weights: 5.006067432696, critic weights: 910.2663292884827, elapsed: 9.56601941982905 minutes\n",
      "episode 290/10000, mean past 10 Reward: 767.7916307894225,actor weights: 34.45609286427498, critic weights: 657.1870750784874, elapsed: 9.99470822016398 minutes\n",
      "episode 300/10000, mean past 10 Reward: 765.957377821667,actor weights: 3.6687782499939203, critic weights: 498.68715131282806, elapsed: 10.425244577725728 minutes\n",
      "episode 310/10000, mean past 10 Reward: 765.7118570793468,actor weights: 20.959500461816788, critic weights: 566.5245575904846, elapsed: 10.855822010835011 minutes\n",
      "episode 320/10000, mean past 10 Reward: 769.5895728696939,actor weights: 12.390935624018312, critic weights: 476.01827819645405, elapsed: 11.28408416112264 minutes\n",
      "episode 330/10000, mean past 10 Reward: 771.1143686489479,actor weights: 5.559033120982349, critic weights: 366.8460923358798, elapsed: 11.713357480367025 minutes\n",
      "episode 340/10000, mean past 10 Reward: 768.4877632520888,actor weights: 16.57050990872085, critic weights: 405.55729622393847, elapsed: 12.142675423622132 minutes\n",
      "episode 350/10000, mean past 10 Reward: 766.423140487767,actor weights: 13.073141545057297, critic weights: 420.70645113289356, elapsed: 12.571425179640451 minutes\n",
      "episode 360/10000, mean past 10 Reward: 768.5953162267214,actor weights: 24.868431359529495, critic weights: 337.63016752898693, elapsed: 13.002081255118052 minutes\n",
      "episode 370/10000, mean past 10 Reward: 771.8850374910915,actor weights: 15.334694176912308, critic weights: 309.8532338887453, elapsed: 13.430934131145477 minutes\n",
      "episode 380/10000, mean past 10 Reward: 766.8761512915223,actor weights: 11.587704036384821, critic weights: 301.76078855991364, elapsed: 13.859317696094513 minutes\n",
      "episode 390/10000, mean past 10 Reward: 768.8639941268284,actor weights: 20.558530818670988, critic weights: 209.66216477751732, elapsed: 14.28759978612264 minutes\n",
      "episode 400/10000, mean past 10 Reward: 767.6407588056961,actor weights: 16.580216839909554, critic weights: 195.08298179507256, elapsed: 14.715780015786489 minutes\n",
      "episode 410/10000, mean past 10 Reward: 768.5589075247484,actor weights: 0.22919155249837786, critic weights: 157.9097627401352, elapsed: 15.145482369263966 minutes\n",
      "episode 420/10000, mean past 10 Reward: 769.4796832075701,actor weights: 1.8527505621314049, critic weights: 184.89033940434456, elapsed: 15.575963278611502 minutes\n",
      "episode 430/10000, mean past 10 Reward: 771.6981535319529,actor weights: 5.287052126601338, critic weights: 123.0948269367218, elapsed: 16.003261383374532 minutes\n",
      "episode 440/10000, mean past 10 Reward: 769.3000361057508,actor weights: 1.7764005330391228, critic weights: 124.50171319395304, elapsed: 16.430756111939747 minutes\n",
      "episode 450/10000, mean past 10 Reward: 770.3198861100922,actor weights: 1.9537470540963113, critic weights: 128.04147100448608, elapsed: 16.85854441722234 minutes\n",
      "episode 460/10000, mean past 10 Reward: 768.260668663567,actor weights: 7.193577099591494, critic weights: 108.26866614818573, elapsed: 17.286074046293894 minutes\n",
      "episode 470/10000, mean past 10 Reward: 769.6367709353987,actor weights: 80.83788058161736, critic weights: 167.58233785629272, elapsed: 17.713418038686118 minutes\n",
      "episode 480/10000, mean past 10 Reward: 769.1082109595384,actor weights: 26.256291597150266, critic weights: 130.84113915264606, elapsed: 18.14093226591746 minutes\n",
      "episode 490/10000, mean past 10 Reward: 769.533552767797,actor weights: 38.728637389838696, critic weights: 138.26122726500034, elapsed: 18.568629944324492 minutes\n",
      "episode 500/10000, mean past 10 Reward: 767.8124505021153,actor weights: 36.30247876048088, critic weights: 90.57571050524712, elapsed: 18.996768192450205 minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 44\u001b[0m\n\u001b[0;32m     39\u001b[0m uav_loc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state[:\u001b[38;5;241m-\u001b[39mQUAD\u001b[38;5;241m.\u001b[39muser_number], (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     42\u001b[0m reward , done \u001b[38;5;241m=\u001b[39m QUAD\u001b[38;5;241m.\u001b[39mmy_reward(uav_loc, scheduling, user_profile)\n\u001b[1;32m---> 44\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(action, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     46\u001b[0m reward_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "reward_acul = np.empty(0)\n",
    "\n",
    "actor_prev.load_state_dict(actor.state_dict())\n",
    "critic_prev.load_state_dict(critic.state_dict())\n",
    "\n",
    "\n",
    "for episode in range(1,EPISODES+1):\n",
    "    state = QUAD.init_state()  # Get initial state\n",
    "    total_reward = 0\n",
    "\n",
    "    actor_weight = 0\n",
    "    critic_weight = 0\n",
    "\n",
    "\n",
    "    # actor_prev = copy.deepcopy(actor)\n",
    "    # critic_prev = copy.deepcopy(critic)\n",
    "\n",
    "    for step in range(1,STEPS_PER_EPISODE+1):\n",
    "\n",
    "        # Select action\n",
    "\n",
    "        state_tensor = torch.FloatTensor(state).to(device)\n",
    "\n",
    "        action_probs= actor(state_tensor)\n",
    "\n",
    "        action = torch.multinomial(action_probs, 1).item()  # Sample action\n",
    "\n",
    "        ###################\n",
    "\n",
    "        uav_loc = np.reshape(state[:-QUAD.user_number], (2,-1))\n",
    "        user_profile = state[-QUAD.user_number:]\n",
    "\n",
    "        scheduling = schedule(user_profile,QUAD.user_loc,uav_loc,QUAD.transmit_pow,QUAD.noise,QUAD.height)\n",
    "\n",
    "        next_state = QUAD.next_state(action,uav_loc,user_profile,scheduling)\n",
    "\n",
    "        uav_loc = np.reshape(next_state[:-QUAD.user_number], (2,-1))\n",
    "\n",
    "\n",
    "        reward , done = QUAD.my_reward(uav_loc, scheduling, user_profile)\n",
    "\n",
    "        state_tensor = torch.tensor(np.array(state), dtype=torch.float32, device=device)\n",
    "        action = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "        reward_tensor = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "        next_tensor = torch.tensor(np.array(next_state), dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "################################ training\n",
    "\n",
    "        # Compute Advantage\n",
    "        value = critic(state_tensor)\n",
    "        next_value = critic(next_tensor).detach()\n",
    "        advantage = reward_tensor + (0.99 * next_value * (not done)) - value\n",
    "\n",
    "        # Update Actor\n",
    "        log_prob = torch.log(action_probs[action])\n",
    "        loss_actor = -log_prob * advantage.detach()\n",
    "        actor_optimizer.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        # Update Critic\n",
    "        loss_critic = advantage.pow(2)\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "\n",
    "################################ end of training\n",
    "\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    reward_acul = np.append(reward_acul,total_reward)\n",
    "\n",
    "    if episode%10 == 0 :\n",
    "        toc = time.time()\n",
    "        print(f\"episode {episode}/{EPISODES}, mean past 10 Reward: {np.mean(reward_acul[-10:])},actor weights: {weight_change(actor,actor_prev)}, critic weights: {weight_change(critic,critic_prev)}, elapsed: {(toc-tic)/60} minutes\")\n",
    "        actor_prev.load_state_dict(actor.state_dict())\n",
    "        critic_prev.load_state_dict(critic.state_dict())\n",
    "\n",
    "    if episode%100 == 0:\n",
    "\n",
    "        check_point = {\n",
    "        'actor_state':actor.state_dict(),\n",
    "        'actor_optimizer': actor_optimizer.state_dict(),\n",
    "        'critic_state':critic.state_dict(),\n",
    "        'critic_optimizer': critic_optimizer.state_dict(),\n",
    "        'episode' : episode\n",
    "        }\n",
    "\n",
    "        torch.save(check_point,\"model.pth\")\n",
    "        np.save(\"reward.npy\",reward_acul)\n",
    "        np.save(f\"episode.npy\",episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d281cb-f461-4ddb-aaa3-69826fde0e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
