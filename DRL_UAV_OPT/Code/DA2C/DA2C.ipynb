{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d2f30f4-4239-43a8-ae20-f8dd008fd879",
   "metadata": {
    "id": "Xwjn8wDNLMnL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea38c8e-cc02-4a6e-99cc-a22c905fd8e5",
   "metadata": {
    "id": "Ja1Kkdl2Qx_F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# all array inputs must be of the type np.array\n",
    "\n",
    "class my_env:\n",
    "    #def __init__(self, uav_number, user_number, size, user_loc, noise, height, transmit_pow, safe_dist): # initializes environment\n",
    "    def __init__(self):\n",
    "        self.uav_number = 2\n",
    "        self.user_number = 5\n",
    "        self.size = np.array([10,10])\n",
    "        self.user_loc = np.array([[-5,-5,5,5,0],[-5,5,-5,5,0]])\n",
    "        self.safe_dist = 5\n",
    "        self.noise = 0.01\n",
    "        self.height = 7\n",
    "        self.transmit_pow = 1\n",
    "        self.observation_space = 2*self.uav_number + self.user_number\n",
    "\n",
    "        ### Parameters\n",
    "        self.actions = 5 # 0: 10 units Left , 1: 10 units up , 2: 10 units right, 3: 10 units down, 4: stay\n",
    "        self.user_profile = 1*np.ones(self.user_number)\n",
    "        ###\n",
    "\n",
    "    def my_reward(self, uav_loc, scheduling, user_prof, agent): # calculate reward\n",
    "        num_agents = np.shape(uav_loc)[1]\n",
    "        num_user = np.shape(self.user_loc)[1]\n",
    "\n",
    "        pref = np.zeros((num_agents,num_user))\n",
    "        pref_rate = np.zeros((num_agents,num_user))\n",
    "        for i in range(np.shape(pref)[0]):\n",
    "            for j in range(np.shape(pref)[1]):\n",
    "                interf = 0\n",
    "                for k in range(np.shape(pref)[0]):\n",
    "                    if k != i:\n",
    "                        interf = interf + self.transmit_pow/(self.height**2 + np.linalg.norm(uav_loc[:,k]-self.user_loc[:,j]))\n",
    "                pref[i,j] = user_prof[j]*np.log2(1 + self.transmit_pow/(self.height**2 + np.linalg.norm(uav_loc[:,i]-self.user_loc[:,j]))/(interf+self.noise))\n",
    "                #pref[i,j] = np.log2(1 + self.transmit_pow/(self.height**2 + np.linalg.norm(uav_loc[:,i]-self.user_loc[:,j]))/(interf+self.noise))\n",
    "\n",
    "\n",
    "        ####### ---- penalty\n",
    "        penalty = 0\n",
    "\n",
    "        for i in range(np.shape(pref)[0]):\n",
    "            for k in range(np.shape(pref)[0]):\n",
    "                if k != i:\n",
    "                    penalty = penalty + (np.linalg.norm(uav_loc[:,k]-uav_loc[:,i]) < self.safe_dist)\n",
    "\n",
    "        # reward = np.sum(np.multiply(scheduling,pref)) - np.log2(1 + self.transmit_pow/(self.height**2)/self.noise )*penalty/2\n",
    "        reward = np.sum(np.multiply(scheduling[agent-1,:],pref[agent-1,:])) - 1000*penalty/4\n",
    "\n",
    "        if penalty != 0:\n",
    "            done =  True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        return reward , done\n",
    "\n",
    "\n",
    "    def next_state(self, action1, uav_loc, user_prof, scheduling):\n",
    "        if action1<5:\n",
    "            action = np.base_repr(action1,base=5,padding = 1)\n",
    "        else :\n",
    "            action = np.base_repr(action1,base=5,padding = 0)\n",
    "\n",
    "        for i in range(len(action)):\n",
    "            if action[i] == \"0\":\n",
    "\n",
    "                uav_loc[:,i] = uav_loc[:,i] + (-2*np.array([1,0]))\n",
    "                uav_loc[0,i] = uav_loc[0,i] + (uav_loc[0,i]<-self.size[0]/2)*(-self.size[0]/2-uav_loc[0,i])\n",
    "\n",
    "            elif action[i] == \"1\":\n",
    "\n",
    "                uav_loc[:,i] = uav_loc[:,i] + (2*np.array([0,1]))\n",
    "                uav_loc[1,i] = uav_loc[1,i] + (uav_loc[1,i]>self.size[1]/2)*(self.size[1]/2-uav_loc[1,i])\n",
    "\n",
    "            elif action[i] == \"2\":\n",
    "\n",
    "                uav_loc[:,i] = uav_loc[:,i] + (2*np.array([1,0]))\n",
    "                uav_loc[0,i] = uav_loc[0,i] + (uav_loc[0,i]>self.size[0]/2)*(self.size[0]/2-uav_loc[0,i])\n",
    "\n",
    "            elif action[i] == \"3\":\n",
    "\n",
    "                uav_loc[:,i] = uav_loc[:,i] + (-2*np.array([0,1]))\n",
    "                uav_loc[1,i] = uav_loc[1,i] + (uav_loc[1,i]<-self.size[1]/2)*(-self.size[1]/2-uav_loc[1,i])\n",
    "\n",
    "            else :\n",
    "                pass\n",
    "\n",
    "        #user_prof = user_prof + (0.1 * ~(np.sum(scheduling,axis=0)>0))\n",
    "        # user_prof = user_prof - (np.min(user_prof)>5)*5\n",
    "        user_prof = 1/(0.9/user_prof + 0.1*np.sum(scheduling,axis=0))\n",
    "\n",
    "        return np.hstack((uav_loc.flatten() , user_prof))\n",
    "\n",
    "    def init_state(self):\n",
    "\n",
    "        while True:\n",
    "            a = np.random.uniform(-self.size[0]/2,self.size[0]/2,size = (1,self.uav_number))\n",
    "            b = np.random.uniform(-self.size[1]/2,self.size[1]/2,size = (1,self.uav_number))\n",
    "            c = np.vstack((a,b))\n",
    "            if np.linalg.norm(c[:,0]-c[:,1])>self.safe_dist:\n",
    "                break\n",
    "        \n",
    " \n",
    "\n",
    "        return np.hstack((a.flatten(),b.flatten(),self.user_profile))\n",
    "\n",
    "\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def schedule(user_prof,user_loc,UAV_loc,transmit_pow,noise,height):\n",
    "\n",
    "\n",
    "    num_agents = np.shape(UAV_loc)[1]\n",
    "    num_user = np.shape(user_loc)[1]\n",
    "\n",
    "    A = cp.Variable((num_agents,num_user))\n",
    "\n",
    "    pref = np.zeros((num_agents,num_user))\n",
    "    for i in range(np.shape(pref)[0]):\n",
    "        for j in range(np.shape(pref)[1]):\n",
    "            interf = 0\n",
    "            for k in range(np.shape(pref)[0]):\n",
    "                if k != i:\n",
    "                    interf = interf + transmit_pow/(height**2 + np.linalg.norm(UAV_loc[:,k]-user_loc[:,j]))\n",
    "            pref[i,j] = user_prof[j]*np.log2(1 + transmit_pow/(height**2 + np.linalg.norm(UAV_loc[:,i]-user_loc[:,j]))/(interf+noise))\n",
    "\n",
    "    objective = cp.Minimize(-1*cp.sum(cp.multiply(A,pref)))\n",
    "\n",
    "    constraints = [A>=0,A<=1]\n",
    "    constraints.append(cp.sum(A,axis = 0) <= 1)\n",
    "    constraints.append(cp.sum(A,axis = 1) <= 1)\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    problem.solve()\n",
    "\n",
    "    return np.round(A.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "226a87ac-2838-4673-b9e3-6bb06a0a5e19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFlGXltnRTI0",
    "outputId": "dc018c48-b7f3-4612-e5d6-8a0905156207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4351a2e-c55f-4744-aed5-ab466b716ebc",
   "metadata": {
    "id": "KcPRGUVgRx9z"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LR = 0.001\n",
    "EPISODES = 10000\n",
    "STEPS_PER_EPISODE = 200\n",
    "\n",
    "QUAD = my_env()\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "def average_weights(model1, model2):\n",
    "    \"\"\"Replace the weights of both models with the average of their weights.\"\"\"\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        param1.data.copy_((param1.data + param2.data) / 2)\n",
    "        param2.data.copy_(param1.data)\n",
    "\n",
    "\n",
    "#Define Actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, action_dim)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        return (F.softmax(self.fc2(x),dim = -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define Critic network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 1)  # Output a single value\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)  # State value estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f4a1383-28f3-40b9-96ef-1ccb06a2b857",
   "metadata": {
    "id": "NQMPBucndagr"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize networks on GPU\n",
    "actor1 = Actor(QUAD.observation_space,QUAD.actions).to(device)\n",
    "critic1 = Critic(QUAD.observation_space).to(device)\n",
    "\n",
    "actor2 = Actor(QUAD.observation_space,QUAD.actions).to(device)\n",
    "critic2 = Critic(QUAD.observation_space).to(device)\n",
    "\n",
    "# for param in actor.parameters():\n",
    "#     if param.ndimension() > 1:  # Check if it's a weight (not a bias)\n",
    "#         torch.nn.init.constant_(param, 0.0001)\n",
    "\n",
    "actor_prev1 = Actor(QUAD.observation_space,QUAD.actions).to(device)\n",
    "critic_prev1 = Critic(QUAD.observation_space).to(device)\n",
    "\n",
    "actor_prev2 = Actor(QUAD.observation_space,QUAD.actions).to(device)\n",
    "critic_prev2 = Critic(QUAD.observation_space).to(device)\n",
    "\n",
    "\n",
    "actor1.apply(weight_init)\n",
    "critic1.apply(weight_init)\n",
    "\n",
    "actor2.apply(weight_init)\n",
    "critic2.apply(weight_init)\n",
    "\n",
    "# Optimizers\n",
    "actor_optimizer1 = optim.Adam(actor1.parameters(), lr=LR)\n",
    "critic_optimizer1 = optim.Adam(critic1.parameters(), lr=LR)\n",
    "\n",
    "actor_optimizer2 = optim.Adam(actor2.parameters(), lr=LR)\n",
    "critic_optimizer2 = optim.Adam(critic2.parameters(), lr=LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea836e63-bdf9-436e-b8bb-840061d5debf",
   "metadata": {
    "id": "2k1NUgQaTaT0"
   },
   "outputs": [],
   "source": [
    "def weight_change(model1,model2):\n",
    "    weight_diff = 0\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        weight_diff += torch.sum(torch.abs(param1 - param2)).item()\n",
    "    return weight_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a790ca-9eb5-404d-9b50-158ea2f62c54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "gKvc1SDLRQKT",
    "outputId": "74c50505-5815-48be-f743-6ace9ae3bb86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 10/10000, mean past 10 Reward: -604.7299235417083,actor1 weights: 43.35863198339939, actor2 weights: 421.59405583143234, critic weights: 40864.04282869212,  elapsed: 0.15850316286087035 minutes\n",
      "episode 20/10000, mean past 10 Reward: 250.42431800349337,actor1 weights: 59.48215910047293, actor2 weights: 66.23610829561949, critic weights: 8164.5203276611865,  elapsed: 0.6094334999720256 minutes\n",
      "episode 30/10000, mean past 10 Reward: 249.14452361671988,actor1 weights: 62.478391490876675, actor2 weights: 62.539916314184666, critic weights: 12371.877602469176,  elapsed: 1.0382891813913981 minutes\n",
      "episode 40/10000, mean past 10 Reward: 460.8278914645737,actor1 weights: 55.46295992285013, actor2 weights: 0.021109493682160974, critic weights: 7191.372971333563,  elapsed: 1.5585967183113099 minutes\n",
      "episode 50/10000, mean past 10 Reward: 769.1567894984844,actor1 weights: 2.208088904619217, actor2 weights: 0.06942390091717243, critic weights: 2291.01672886312,  elapsed: 2.160538677374522 minutes\n",
      "episode 60/10000, mean past 10 Reward: 768.9411227235562,actor1 weights: 2.470959845930338, actor2 weights: 0.39881292916834354, critic weights: 2033.3042755499482,  elapsed: 2.763309880097707 minutes\n",
      "episode 70/10000, mean past 10 Reward: 768.2751805361888,actor1 weights: 0.459943613037467, actor2 weights: 1.3082759361714125, critic weights: 1972.019969008863,  elapsed: 3.396278448899587 minutes\n",
      "episode 80/10000, mean past 10 Reward: 768.7897701202794,actor1 weights: 5.882962998002768, actor2 weights: 7.7712001614272594, critic weights: 2317.313807055354,  elapsed: 4.027131442228953 minutes\n",
      "episode 90/10000, mean past 10 Reward: 768.464106302646,actor1 weights: 24.412519443780184, actor2 weights: 4.167469127103686, critic weights: 2015.0742683410645,  elapsed: 4.6569424311319985 minutes\n",
      "episode 100/10000, mean past 10 Reward: 769.4564040532659,actor1 weights: 2.6862558173015714, actor2 weights: 2.4227981101721525, critic weights: 1749.454410329461,  elapsed: 5.2818354288736975 minutes\n",
      "episode 110/10000, mean past 10 Reward: 771.1066085651224,actor1 weights: 38.87392068654299, actor2 weights: 1.517553560435772, critic weights: 999.9989904165268,  elapsed: 5.917686605453492 minutes\n",
      "episode 120/10000, mean past 10 Reward: 771.739998325261,actor1 weights: 65.56642188131809, actor2 weights: 8.175734484568238, critic weights: 1384.1482122540474,  elapsed: 6.541101257006328 minutes\n",
      "episode 130/10000, mean past 10 Reward: 768.9538569566987,actor1 weights: 0.20687537733465433, actor2 weights: 2.7225188203155994, critic weights: 1030.6642264127731,  elapsed: 7.1586289246877035 minutes\n",
      "episode 140/10000, mean past 10 Reward: 766.7375853376313,actor1 weights: 0.26040005683898926, actor2 weights: 3.8885776810348034, critic weights: 816.235719025135,  elapsed: 7.771871141592661 minutes\n",
      "episode 150/10000, mean past 10 Reward: 769.0921590560666,actor1 weights: 1.8056853115558624, actor2 weights: 4.716032024472952, critic weights: 870.6233233213425,  elapsed: 8.386881935596467 minutes\n",
      "episode 160/10000, mean past 10 Reward: 770.7489749269255,actor1 weights: 13.484880778938532, actor2 weights: 16.29685351997614, critic weights: 493.0464061200619,  elapsed: 9.001189891497294 minutes\n",
      "episode 170/10000, mean past 10 Reward: 766.6979002871171,actor1 weights: 0.9887926690280437, actor2 weights: 2.862673133611679, critic weights: 556.2095615565777,  elapsed: 9.615981260935465 minutes\n",
      "episode 180/10000, mean past 10 Reward: 770.630447015493,actor1 weights: 2.529082573018968, actor2 weights: 3.7264859825372696, critic weights: 567.0681725740433,  elapsed: 10.228937391440073 minutes\n",
      "episode 190/10000, mean past 10 Reward: 770.3618208719175,actor1 weights: 6.422292985953391, actor2 weights: 13.746796816587448, critic weights: 488.6950621306896,  elapsed: 10.841155139605204 minutes\n",
      "episode 200/10000, mean past 10 Reward: 766.640822238217,actor1 weights: 7.867575348354876, actor2 weights: 3.0636288691312075, critic weights: 219.50449496507645,  elapsed: 11.453444504737854 minutes\n",
      "episode 210/10000, mean past 10 Reward: 769.3521497291201,actor1 weights: 41.60555826500058, actor2 weights: 10.778685104101896, critic weights: 433.21567791700363,  elapsed: 12.065205574035645 minutes\n",
      "episode 220/10000, mean past 10 Reward: 770.8207169646109,actor1 weights: 30.668074084445834, actor2 weights: 1.2959625106304884, critic weights: 444.04513424634933,  elapsed: 12.735597797234853 minutes\n",
      "episode 230/10000, mean past 10 Reward: 770.0630719000902,actor1 weights: 29.803869031369686, actor2 weights: 11.990347228944302, critic weights: 180.4867558181286,  elapsed: 13.448116409778596 minutes\n",
      "episode 240/10000, mean past 10 Reward: 767.5133649245538,actor1 weights: 42.57720335572958, actor2 weights: 5.71945359185338, critic weights: 270.6427252292633,  elapsed: 14.059374197324116 minutes\n",
      "episode 250/10000, mean past 10 Reward: 774.389868773227,actor1 weights: 19.63722786307335, actor2 weights: 7.749216424301267, critic weights: 211.670441955328,  elapsed: 14.668929982185364 minutes\n",
      "episode 260/10000, mean past 10 Reward: 767.9834500729241,actor1 weights: 7.433899296447635, actor2 weights: 32.5042514950037, critic weights: 188.56018751859665,  elapsed: 15.279538838068644 minutes\n",
      "episode 270/10000, mean past 10 Reward: 768.5934638184739,actor1 weights: 6.954325009137392, actor2 weights: 11.920920472592115, critic weights: 149.05130073428154,  elapsed: 15.888685782750448 minutes\n",
      "episode 280/10000, mean past 10 Reward: 766.3382507624709,actor1 weights: 39.19800391793251, actor2 weights: 53.68830193579197, critic weights: 95.56036078929901,  elapsed: 16.49959001541138 minutes\n",
      "episode 290/10000, mean past 10 Reward: 771.3638210171241,actor1 weights: 30.317964650690556, actor2 weights: 0.0036024481523782015, critic weights: 264.98371410369873,  elapsed: 17.11277419726054 minutes\n",
      "episode 300/10000, mean past 10 Reward: 768.7852650967806,actor1 weights: 0.029959478182718158, actor2 weights: 0.0048634931445121765, critic weights: 162.51982310414314,  elapsed: 17.78303321202596 minutes\n",
      "episode 310/10000, mean past 10 Reward: 769.660663859305,actor1 weights: 0.014797897660173476, actor2 weights: 0.008308718912303448, critic weights: 108.41790755093098,  elapsed: 18.482296228408813 minutes\n",
      "episode 320/10000, mean past 10 Reward: 769.5152369753989,actor1 weights: 0.23114356258884072, actor2 weights: 0.007321877172216773, critic weights: 127.09998553991318,  elapsed: 19.14567770163218 minutes\n",
      "episode 330/10000, mean past 10 Reward: 768.460532907194,actor1 weights: 0.09282065648585558, actor2 weights: 0.0030937890987843275, critic weights: 118.3343755453825,  elapsed: 19.771517976125082 minutes\n",
      "episode 340/10000, mean past 10 Reward: 769.7640120213609,actor1 weights: 0.12930989358574152, actor2 weights: 0.018523441161960363, critic weights: 66.96436549723148,  elapsed: 20.38146203358968 minutes\n",
      "episode 350/10000, mean past 10 Reward: 769.8945852059384,actor1 weights: 0.16966315731406212, actor2 weights: 0.004825556185096502, critic weights: 128.43211837112904,  elapsed: 20.992644480864207 minutes\n",
      "episode 360/10000, mean past 10 Reward: 769.4540293582404,actor1 weights: 0.6091714708600193, actor2 weights: 0.007294528419151902, critic weights: 73.3960205540061,  elapsed: 21.6045352657636 minutes\n",
      "episode 370/10000, mean past 10 Reward: 765.631181530213,actor1 weights: 0.07709196931682527, actor2 weights: 0.005478252191096544, critic weights: 79.56363335251808,  elapsed: 22.213412423928578 minutes\n",
      "episode 380/10000, mean past 10 Reward: 767.6996966615079,actor1 weights: 0.3954762842040509, actor2 weights: 0.015516686020419002, critic weights: 70.40575981140137,  elapsed: 22.823569575945537 minutes\n",
      "episode 390/10000, mean past 10 Reward: 767.5263191521265,actor1 weights: 0.840265286155045, actor2 weights: 0.0025124395033344626, critic weights: 41.65400418639183,  elapsed: 23.428795274098714 minutes\n",
      "episode 400/10000, mean past 10 Reward: 768.0956595419755,actor1 weights: 0.024992921855300665, actor2 weights: 0.010505213402211666, critic weights: 83.70427709817886,  elapsed: 24.085781280199686 minutes\n",
      "episode 410/10000, mean past 10 Reward: 771.4446175127243,actor1 weights: 0.33078622352331877, actor2 weights: 0.00538105983287096, critic weights: 131.96053740382195,  elapsed: 24.733490431308745 minutes\n",
      "episode 420/10000, mean past 10 Reward: 769.0591819923171,actor1 weights: 0.06117534381337464, actor2 weights: 0.0039354353211820126, critic weights: 66.25784240663052,  elapsed: 25.338322695096334 minutes\n",
      "episode 430/10000, mean past 10 Reward: 767.6726814101228,actor1 weights: 0.2730285811703652, actor2 weights: 0.029213527915999293, critic weights: 59.73608007282019,  elapsed: 25.9459064523379 minutes\n",
      "episode 440/10000, mean past 10 Reward: 768.0293065288868,actor1 weights: 0.5643616814631969, actor2 weights: 0.01441510091535747, critic weights: 55.65214768052101,  elapsed: 26.550400873025257 minutes\n",
      "episode 450/10000, mean past 10 Reward: 768.273477555405,actor1 weights: 0.10072918981313705, actor2 weights: 0.0032225927570834756, critic weights: 63.333987921476364,  elapsed: 27.1557520031929 minutes\n",
      "episode 460/10000, mean past 10 Reward: 771.2319252310483,actor1 weights: 0.10371767519973218, actor2 weights: 0.001985309994779527, critic weights: 59.66405598819256,  elapsed: 27.762059100468953 minutes\n",
      "episode 470/10000, mean past 10 Reward: 767.5859510095328,actor1 weights: 0.022055379580706358, actor2 weights: 0.00328495423309505, critic weights: 61.55536625534296,  elapsed: 28.367868665854136 minutes\n",
      "episode 480/10000, mean past 10 Reward: 769.4297082893531,actor1 weights: 0.12248637620359659, actor2 weights: 0.0036370016168802977, critic weights: 65.1512008793652,  elapsed: 28.99849446217219 minutes\n",
      "episode 490/10000, mean past 10 Reward: 767.4435787415997,actor1 weights: 0.19868552149273455, actor2 weights: 0.0030492839869111776, critic weights: 71.53867276757956,  elapsed: 29.655542333920796 minutes\n",
      "episode 500/10000, mean past 10 Reward: 770.4549369837086,actor1 weights: 0.21434877323918045, actor2 weights: 0.013582199113443494, critic weights: 66.37912721931934,  elapsed: 30.29525272846222 minutes\n",
      "episode 510/10000, mean past 10 Reward: 767.258083541479,actor1 weights: 0.02352049178443849, actor2 weights: 0.0023104159627109766, critic weights: 40.72169178724289,  elapsed: 30.962812129656474 minutes\n",
      "episode 520/10000, mean past 10 Reward: 769.9260472499814,actor1 weights: 2.4668754851445556, actor2 weights: 0.009536737110465765, critic weights: 66.3888860270381,  elapsed: 31.6370103319486 minutes\n",
      "episode 530/10000, mean past 10 Reward: 769.0062570793017,actor1 weights: 5.903083056211472, actor2 weights: 0.010642791166901588, critic weights: 45.57527506351471,  elapsed: 32.2417281071345 minutes\n",
      "episode 540/10000, mean past 10 Reward: 769.1526262794146,actor1 weights: 0.08436401223298162, actor2 weights: 0.006448300555348396, critic weights: 44.71223023533821,  elapsed: 32.8489821155866 minutes\n",
      "episode 550/10000, mean past 10 Reward: 767.2323036870627,actor1 weights: 0.041115303756669164, actor2 weights: 0.004245804855599999, critic weights: 37.95336023718119,  elapsed: 33.455733847618106 minutes\n",
      "episode 560/10000, mean past 10 Reward: 767.9309795644525,actor1 weights: 0.008793877437710762, actor2 weights: 0.005861695157364011, critic weights: 55.83283352851868,  elapsed: 34.06208622455597 minutes\n",
      "episode 570/10000, mean past 10 Reward: 771.2529339624119,actor1 weights: 0.055370095069520175, actor2 weights: 0.0031231387984007597, critic weights: 37.53490374376997,  elapsed: 34.672594451904295 minutes\n",
      "episode 580/10000, mean past 10 Reward: 769.5764298651409,actor1 weights: 0.03249301272444427, actor2 weights: 0.004292004625312984, critic weights: 49.361129343509674,  elapsed: 35.27868631680806 minutes\n",
      "episode 590/10000, mean past 10 Reward: 770.5972443601386,actor1 weights: 0.02393805020255968, actor2 weights: 0.002120393211953342, critic weights: 55.370120882987976,  elapsed: 35.88560278813044 minutes\n",
      "episode 600/10000, mean past 10 Reward: 770.0290272852171,actor1 weights: 0.03277209703810513, actor2 weights: 0.004372634459286928, critic weights: 46.495836198329926,  elapsed: 36.491306364536285 minutes\n",
      "episode 610/10000, mean past 10 Reward: 768.9738276192703,actor1 weights: 0.02916383056435734, actor2 weights: 0.003908942802809179, critic weights: 60.931176006793976,  elapsed: 37.102722803751625 minutes\n",
      "episode 620/10000, mean past 10 Reward: 770.1497536770173,actor1 weights: 0.11892513418570161, actor2 weights: 0.007737991865724325, critic weights: 54.33170500397682,  elapsed: 37.71108034849167 minutes\n",
      "episode 630/10000, mean past 10 Reward: 767.1169339911709,actor1 weights: 0.05097201943863183, actor2 weights: 0.003172756521962583, critic weights: 44.01870787143707,  elapsed: 38.31645065148671 minutes\n",
      "episode 640/10000, mean past 10 Reward: 770.5232071054403,actor1 weights: 0.07310984213836491, actor2 weights: 0.007456840481609106, critic weights: 50.77521911263466,  elapsed: 38.9234525124232 minutes\n",
      "episode 650/10000, mean past 10 Reward: 768.2739277469362,actor1 weights: 0.10921628726646304, actor2 weights: 0.007352882064878941, critic weights: 58.77341789752245,  elapsed: 39.529608794053395 minutes\n",
      "episode 660/10000, mean past 10 Reward: 768.3733484495713,actor1 weights: 0.05980748275760561, actor2 weights: 0.01775804697535932, critic weights: 35.83655035495758,  elapsed: 40.13798201084137 minutes\n",
      "episode 670/10000, mean past 10 Reward: 767.8258032387487,actor1 weights: 0.07360668363980949, actor2 weights: 0.015081129968166351, critic weights: 33.37386071681976,  elapsed: 40.74601505200068 minutes\n",
      "episode 680/10000, mean past 10 Reward: 769.0449389091706,actor1 weights: 0.09780816792044789, actor2 weights: 0.005429170094430447, critic weights: 41.52670434117317,  elapsed: 41.35001949071884 minutes\n",
      "episode 690/10000, mean past 10 Reward: 766.0619436099066,actor1 weights: 0.05233064200729132, actor2 weights: 0.0059834124986082315, critic weights: 43.55857685953379,  elapsed: 41.958332391579944 minutes\n",
      "episode 700/10000, mean past 10 Reward: 771.311096489679,actor1 weights: 0.03556951170321554, actor2 weights: 0.0020226145861670375, critic weights: 48.85662431642413,  elapsed: 42.56338965495427 minutes\n",
      "episode 710/10000, mean past 10 Reward: 766.0045809353032,actor1 weights: 0.03264370351098478, actor2 weights: 0.002974774339236319, critic weights: 33.63739335536957,  elapsed: 43.17262829144796 minutes\n",
      "episode 720/10000, mean past 10 Reward: 769.3296601983893,actor1 weights: 0.03078900248510763, actor2 weights: 0.005336486967280507, critic weights: 42.771537482738495,  elapsed: 43.780212171872456 minutes\n",
      "episode 730/10000, mean past 10 Reward: 771.330056717075,actor1 weights: 0.024232795578427613, actor2 weights: 0.010057168081402779, critic weights: 43.545700285583735,  elapsed: 44.38560935258865 minutes\n",
      "episode 740/10000, mean past 10 Reward: 769.8572993931631,actor1 weights: 0.05305569351185113, actor2 weights: 0.006392585346475244, critic weights: 46.977297492325306,  elapsed: 44.995538588364916 minutes\n",
      "episode 750/10000, mean past 10 Reward: 768.7018785689962,actor1 weights: 0.1121555027202703, actor2 weights: 0.018805789295583963, critic weights: 26.20916971564293,  elapsed: 45.60441722472509 minutes\n",
      "episode 760/10000, mean past 10 Reward: 767.2208601370047,actor1 weights: 0.08727114251814783, actor2 weights: 0.003225518623366952, critic weights: 31.8999984562397,  elapsed: 46.21007894277572 minutes\n",
      "episode 770/10000, mean past 10 Reward: 768.9777616164283,actor1 weights: 0.02121778967557475, actor2 weights: 0.001974778133444488, critic weights: 40.30700623989105,  elapsed: 46.81735683282216 minutes\n",
      "episode 780/10000, mean past 10 Reward: 770.7956442620728,actor1 weights: 0.06575331522617489, actor2 weights: 0.0024838815443217754, critic weights: 35.676353700459,  elapsed: 47.42295605341594 minutes\n",
      "episode 790/10000, mean past 10 Reward: 769.3643769025546,actor1 weights: 0.03558278229320422, actor2 weights: 0.009013133589178324, critic weights: 42.28389201313257,  elapsed: 48.03636499643326 minutes\n",
      "episode 800/10000, mean past 10 Reward: 767.0403253073689,actor1 weights: 0.037062182324007154, actor2 weights: 0.00025843271578196436, critic weights: 34.507796972990036,  elapsed: 48.646213773886366 minutes\n",
      "episode 810/10000, mean past 10 Reward: 769.2843985986176,actor1 weights: 0.057707905711140484, actor2 weights: 0.016758799785748124, critic weights: 30.243533313274384,  elapsed: 49.255566950639086 minutes\n",
      "episode 820/10000, mean past 10 Reward: 770.5474483645967,actor1 weights: 0.04249117407016456, actor2 weights: 0.010326910763978958, critic weights: 36.480157017707825,  elapsed: 49.8642009417216 minutes\n",
      "episode 830/10000, mean past 10 Reward: 768.5353317359077,actor1 weights: 0.046093224780634046, actor2 weights: 0.01427150028757751, critic weights: 26.494840264320374,  elapsed: 50.47078349987666 minutes\n",
      "episode 840/10000, mean past 10 Reward: 768.6889660369956,actor1 weights: 0.05187110271072015, actor2 weights: 0.005065863952040672, critic weights: 45.049780217930675,  elapsed: 51.07908472617467 minutes\n",
      "episode 850/10000, mean past 10 Reward: 767.0428197830873,actor1 weights: 0.01768245064886287, actor2 weights: 6.174697773531079e-06, critic weights: 26.064313989132643,  elapsed: 51.685595973332724 minutes\n",
      "episode 860/10000, mean past 10 Reward: 769.8799126154148,actor1 weights: 0.1340770919341594, actor2 weights: 0.02393706701695919, critic weights: 29.440836049616337,  elapsed: 52.294781200091045 minutes\n",
      "episode 870/10000, mean past 10 Reward: 770.3424181132538,actor1 weights: 0.08160516136558726, actor2 weights: 0.005815121461637318, critic weights: 33.63815604150295,  elapsed: 52.903264327843985 minutes\n",
      "episode 880/10000, mean past 10 Reward: 768.9684405015539,actor1 weights: 0.0992351251479704, actor2 weights: 0.009032403235323727, critic weights: 43.986617147922516,  elapsed: 53.51057709058126 minutes\n",
      "episode 890/10000, mean past 10 Reward: 770.571217334687,actor1 weights: 0.04299395138514228, actor2 weights: 0.011194988270290196, critic weights: 47.393522560596466,  elapsed: 54.11814725399017 minutes\n",
      "episode 900/10000, mean past 10 Reward: 769.7490189356045,actor1 weights: 0.019767741207033396, actor2 weights: 0.004911171272397041, critic weights: 33.024911377578974,  elapsed: 54.73667416572571 minutes\n",
      "episode 910/10000, mean past 10 Reward: 769.6517156771304,actor1 weights: 0.09128890975262038, actor2 weights: 0.005159534048289061, critic weights: 20.938093453645706,  elapsed: 55.34822127024333 minutes\n",
      "episode 920/10000, mean past 10 Reward: 768.6453485440804,actor1 weights: 0.05809712581685744, actor2 weights: 0.029124278342351317, critic weights: 26.84223183989525,  elapsed: 55.954621680577596 minutes\n",
      "episode 930/10000, mean past 10 Reward: 769.4400286861811,actor1 weights: 0.1238160440698266, actor2 weights: 0.0029481530655175447, critic weights: 21.215743258595467,  elapsed: 56.566310838858286 minutes\n",
      "episode 940/10000, mean past 10 Reward: 767.134812078465,actor1 weights: 0.037299238218110986, actor2 weights: 0.0021220624912530184, critic weights: 20.49987637810409,  elapsed: 57.175112561384836 minutes\n",
      "episode 950/10000, mean past 10 Reward: 769.3577177868476,actor1 weights: 0.10062294668750837, actor2 weights: 0.005904925637878478, critic weights: 27.952912035165355,  elapsed: 57.78210370143255 minutes\n",
      "episode 960/10000, mean past 10 Reward: 768.5326874441528,actor1 weights: 0.06817647157004103, actor2 weights: 0.001325975637882948, critic weights: 24.897744765505195,  elapsed: 58.39228235880534 minutes\n",
      "episode 970/10000, mean past 10 Reward: 768.1629315225478,actor1 weights: 0.09578174317721277, actor2 weights: 0.006886057439260185, critic weights: 32.66361617296934,  elapsed: 59.00087036689123 minutes\n",
      "episode 980/10000, mean past 10 Reward: 767.5506124810381,actor1 weights: 0.18276634032372385, actor2 weights: 0.0032525171991437674, critic weights: 30.38774448633194,  elapsed: 59.61205660104751 minutes\n",
      "episode 990/10000, mean past 10 Reward: 768.9877316351846,actor1 weights: 0.03559487569145858, actor2 weights: 0.0025068121030926704, critic weights: 33.52386295795441,  elapsed: 60.21917703151703 minutes\n",
      "episode 1000/10000, mean past 10 Reward: 768.4316282087781,actor1 weights: 0.19123595842393115, actor2 weights: 0.013496373663656414, critic weights: 31.578413411974907,  elapsed: 60.83152576287588 minutes\n",
      "episode 1010/10000, mean past 10 Reward: 770.6237523914855,actor1 weights: 0.03455490188207477, actor2 weights: 0.015575040131807327, critic weights: 32.416235238313675,  elapsed: 61.44515865246455 minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 45\u001b[0m\n\u001b[0;32m     40\u001b[0m next_state \u001b[38;5;241m=\u001b[39m QUAD\u001b[38;5;241m.\u001b[39mnext_state(action,uav_loc,user_profile,scheduling)\n\u001b[0;32m     42\u001b[0m uav_loc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state[:\u001b[38;5;241m-\u001b[39mQUAD\u001b[38;5;241m.\u001b[39muser_number], (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 45\u001b[0m reward1 , done1 \u001b[38;5;241m=\u001b[39m \u001b[43mQUAD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmy_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43muav_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m reward2 , done2 \u001b[38;5;241m=\u001b[39m QUAD\u001b[38;5;241m.\u001b[39mmy_reward(uav_loc, scheduling, user_profile,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     47\u001b[0m reward \u001b[38;5;241m=\u001b[39m reward1 \u001b[38;5;241m+\u001b[39m reward2\n",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m, in \u001b[0;36mmy_env.my_reward\u001b[1;34m(self, uav_loc, scheduling, user_prof, agent)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(pref)[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m i:\n\u001b[1;32m---> 35\u001b[0m                 interf \u001b[38;5;241m=\u001b[39m interf \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransmit_pow\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43muav_loc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_loc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     36\u001b[0m         pref[i,j] \u001b[38;5;241m=\u001b[39m user_prof[j]\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog2(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransmit_pow\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(uav_loc[:,i]\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_loc[:,j]))\u001b[38;5;241m/\u001b[39m(interf\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise))\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;66;03m#pref[i,j] = np.log2(1 + self.transmit_pow/(self.height**2 + np.linalg.norm(uav_loc[:,i]-self.user_loc[:,j]))/(interf+self.noise))\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m####### ---- penalty\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_temp\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2796\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2794\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m x_imag\u001b[38;5;241m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2796\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2797\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "reward_acul = np.empty(0)\n",
    "\n",
    "actor_prev1.load_state_dict(actor1.state_dict())\n",
    "critic_prev1.load_state_dict(critic1.state_dict())\n",
    "\n",
    "actor_prev2.load_state_dict(actor1.state_dict())\n",
    "critic_prev2.load_state_dict(critic1.state_dict())\n",
    "\n",
    "\n",
    "for episode in range(1,EPISODES+1):\n",
    "    state = QUAD.init_state()  # Get initial state\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    for step in range(1,STEPS_PER_EPISODE+1):\n",
    "\n",
    "        # Select action\n",
    "\n",
    "        state_tensor = torch.FloatTensor(state).to(device)\n",
    "\n",
    "        action_probs1= actor1(state_tensor)\n",
    "\n",
    "        action1 = torch.multinomial(action_probs1, 1).item()  # Sample action\n",
    "\n",
    "        action_probs2= actor2(state_tensor)\n",
    "\n",
    "        action2 = torch.multinomial(action_probs2, 1).item()  # Sample action\n",
    "        \n",
    "###############################\n",
    "\n",
    "        uav_loc = np.reshape(state[:-QUAD.user_number], (2,-1))\n",
    "        user_profile = state[-QUAD.user_number:]\n",
    "\n",
    "        scheduling = schedule(user_profile,QUAD.user_loc,uav_loc,QUAD.transmit_pow,QUAD.noise,QUAD.height)\n",
    "\n",
    "        action = 5*action1+action2\n",
    "\n",
    "        next_state = QUAD.next_state(action,uav_loc,user_profile,scheduling)\n",
    "\n",
    "        uav_loc = np.reshape(next_state[:-QUAD.user_number], (2,-1))\n",
    "\n",
    "\n",
    "        reward1 , done1 = QUAD.my_reward(uav_loc, scheduling, user_profile,1)\n",
    "        reward2 , done2 = QUAD.my_reward(uav_loc, scheduling, user_profile,2)\n",
    "        reward = reward1 + reward2\n",
    "\n",
    "        state_tensor = torch.tensor(np.array(state), dtype=torch.float32, device=device)\n",
    "        action1 = torch.tensor(action1, dtype=torch.int64, device=device)\n",
    "        action2 = torch.tensor(action2, dtype=torch.int64, device=device)\n",
    "        reward_tensor1 = torch.tensor(reward1, dtype=torch.float32, device=device)\n",
    "        reward_tensor2 = torch.tensor(reward2, dtype=torch.float32, device=device)\n",
    "        next_tensor = torch.tensor(np.array(next_state), dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "################################ training\n",
    "\n",
    "        # Compute Advantage\n",
    "        value1 = critic1(state_tensor)\n",
    "        value2 = critic2(state_tensor)\n",
    "        next_value1 = critic1(next_tensor).detach()\n",
    "        next_value2 = critic2(next_tensor).detach()\n",
    "        advantage1 = reward_tensor1 + (GAMMA * next_value1 * (not done1)) - value1\n",
    "        advantage2 = reward_tensor2 + (GAMMA * next_value2 * (not done2)) - value2\n",
    "\n",
    "        # Update Actor 1\n",
    "        log_prob1 = torch.log(action_probs1[action1])\n",
    "        loss_actor1 = -log_prob1 * advantage1.detach()\n",
    "        actor_optimizer1.zero_grad()\n",
    "        loss_actor1.backward()\n",
    "        actor_optimizer1.step()\n",
    "\n",
    "        # Update Actor 2\n",
    "        log_prob2 = torch.log(action_probs2[action2])\n",
    "        loss_actor2 = -log_prob2 * advantage2.detach()\n",
    "        actor_optimizer2.zero_grad()\n",
    "        loss_actor2.backward()\n",
    "        actor_optimizer2.step()\n",
    "\n",
    "        # Update Critic 1\n",
    "        loss_critic1 = advantage1.pow(2)\n",
    "        critic_optimizer1.zero_grad()\n",
    "        loss_critic1.backward()\n",
    "        critic_optimizer1.step()\n",
    "\n",
    "        # Update Critic 2\n",
    "        loss_critic2 = advantage2.pow(2)\n",
    "        critic_optimizer2.zero_grad()\n",
    "        loss_critic2.backward()\n",
    "        critic_optimizer2.step()\n",
    "\n",
    "        # consensus on critics \n",
    "\n",
    "        average_weights(critic1, critic2)\n",
    "\n",
    "################################ end of training\n",
    "\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "        if done1:\n",
    "            break\n",
    "\n",
    "\n",
    "    reward_acul = np.append(reward_acul,total_reward)\n",
    "\n",
    "    if episode%10 == 0 :\n",
    "        toc = time.time()\n",
    "        print(f\"episode {episode}/{EPISODES}, mean past 10 Reward: {np.mean(reward_acul[-10:])},actor1 weights: {weight_change(actor1,actor_prev1)}, actor2 weights: {weight_change(actor2,actor_prev2)}, critic weights: {weight_change(critic1,critic_prev1)},  elapsed: {(toc-tic)/60} minutes\")\n",
    "        actor_prev1.load_state_dict(actor1.state_dict())\n",
    "        critic_prev1.load_state_dict(critic1.state_dict())\n",
    "\n",
    "        actor_prev2.load_state_dict(actor2.state_dict())\n",
    "        critic_prev2.load_state_dict(critic2.state_dict())\n",
    "\n",
    "    if episode%100 == 0:\n",
    "\n",
    "        check_point = {\n",
    "        'actor1_state':actor1.state_dict(),\n",
    "        'actor1_optimizer': actor_optimizer1.state_dict(),\n",
    "        'critic1_state':critic1.state_dict(),\n",
    "        'critic1_optimizer': critic_optimizer1.state_dict(),\n",
    "        'actor2_state':actor2.state_dict(),\n",
    "        'actor2_optimizer': actor_optimizer2.state_dict(),\n",
    "        'critic2_state':critic2.state_dict(),\n",
    "        'critic2_optimizer': critic_optimizer2.state_dict(),\n",
    "        'episode' : episode\n",
    "        }\n",
    "\n",
    "        torch.save(check_point,\"model.pth\")\n",
    "        np.save(\"reward.npy\",reward_acul)\n",
    "        np.save(f\"episode.npy\",episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d281cb-f461-4ddb-aaa3-69826fde0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "a = np.load(\"reward.npy\")\n",
    "\n",
    "# Define the window size for the moving average\n",
    "window_size = 10\n",
    "\n",
    "# Calculate the moving average using pandas' rolling method\n",
    "moving_avg = pd.Series(a).rolling(window=window_size).mean()\n",
    "\n",
    "# Convert the result back to a NumPy array (if needed)\n",
    "moving_avg_np = moving_avg.to_numpy()\n",
    "\n",
    "plt.figure(figsize=(30, 8))\n",
    "\n",
    "plt.plot(moving_avg_np)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07685824-3671-48e4-af3f-1a32b279a46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
